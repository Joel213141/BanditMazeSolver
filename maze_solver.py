{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd98c319-53ed-4347-8ce8-9078e4ae0590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be int64, actual type: int32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 110\u001b[0m\n\u001b[0;32m    107\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 110\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 56\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m     state_idx \u001b[38;5;241m=\u001b[39m states_dic[\u001b[38;5;28mtuple\u001b[39m(state\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))]\n\u001b[0;32m     54\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q_table[state_idx, :])  \u001b[38;5;66;03m# Exploit learned values\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m new_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Update Q-table for Q(s,a)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m state_idx \u001b[38;5;241m=\u001b[39m states_dic[\u001b[38;5;28mtuple\u001b[39m(state\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "# maze_solver.py\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_maze\n",
    "\n",
    "def main():\n",
    "    # Initialize the environment\n",
    "    env = gym.make(\"maze-sample-5x5-v0\")\n",
    "\n",
    "    # Create a mapping from state coordinates to state indices\n",
    "    states_dic = {}\n",
    "    count = 0\n",
    "    maze_size = env.maze_size\n",
    "\n",
    "    for i in range(maze_size[0]):\n",
    "        for j in range(maze_size[1]):\n",
    "            states_dic[(i, j)] = count\n",
    "            count += 1\n",
    "\n",
    "    n_states = count\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    # Initialize the Q-table to zeros\n",
    "    Q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "    # Parameters\n",
    "    n_episodes = 500\n",
    "    max_iter_episode = 100\n",
    "    learning_rate = 0.1\n",
    "    gamma = 0.99  # Discount factor\n",
    "\n",
    "    # Exploration parameters\n",
    "    max_epsilon = 1.0\n",
    "    min_epsilon = 0.01\n",
    "    decay_rate = 0.005  # Exponential decay rate for exploration probability\n",
    "\n",
    "    rewards_all_episodes = []\n",
    "\n",
    "    # Training loop\n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rewards_current_episode = 0\n",
    "\n",
    "        for step in range(max_iter_episode):\n",
    "            # Exploration-exploitation trade-off\n",
    "            exploration_rate_threshold = min_epsilon + \\\n",
    "                (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "            if np.random.uniform(0, 1) < exploration_rate_threshold:\n",
    "                action = env.action_space.sample()  # Explore action space\n",
    "            else:\n",
    "                state_idx = states_dic[tuple(state.astype(int))]\n",
    "                action = np.argmax(Q_table[state_idx, :])  # Exploit learned values\n",
    "\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update Q-table for Q(s,a)\n",
    "            state_idx = states_dic[tuple(state.astype(int))]\n",
    "            new_state_idx = states_dic[tuple(new_state.astype(int))]\n",
    "            Q_table[state_idx, action] = Q_table[state_idx, action] * (1 - learning_rate) + \\\n",
    "                learning_rate * (reward + gamma * np.max(Q_table[new_state_idx, :]))\n",
    "\n",
    "            state = new_state\n",
    "            rewards_current_episode += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "        # Print rewards every 100 episodes\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode: {episode + 1}, Reward: {rewards_current_episode}\")\n",
    "\n",
    "    # Calculate and print the average reward per hundred episodes\n",
    "    rewards_per_hundred_episodes = np.split(np.array(rewards_all_episodes), n_episodes / 100)\n",
    "    count = 100\n",
    "\n",
    "    print(\"\\nAverage reward per hundred episodes:\")\n",
    "    for r in rewards_per_hundred_episodes:\n",
    "        print(f\"Episode {count}: Average Reward: {sum(r) / 100}\")\n",
    "        count += 100\n",
    "\n",
    "    # Print the updated Q-table\n",
    "    print(\"\\nQ-table:\")\n",
    "    print(Q_table)\n",
    "\n",
    "    # Testing the agent\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_rewards = 0\n",
    "\n",
    "    print(\"\\nTesting the agent...\\n\")\n",
    "    env.render()\n",
    "    for step in range(max_iter_episode):\n",
    "        state_idx = states_dic[tuple(state.astype(int))]\n",
    "        action = np.argmax(Q_table[state_idx, :])\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        total_rewards += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(f\"\\nReached the goal in {step + 1} steps with total rewards {total_rewards}\")\n",
    "            break\n",
    "        state = new_state\n",
    "\n",
    "    env.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e38028-ea87-4db8-86a3-0dbbebc036a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
